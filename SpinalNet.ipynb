{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R8_SYWhzroVE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset_path = \"/content/drive/MyDrive/Pre_processing/train\"\n",
        "# dataset_path = '/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/new_train/train'\n",
        "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# 레이블 리스트 생성\n",
        "labels = [label for _, label in dataset.samples]\n",
        "\n",
        "# 계층적 분할 생성\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
        "\n",
        "# 인덱스 분할\n",
        "train_idx, val_idx = next(sss.split(labels, labels))\n",
        "\n",
        "# 해당 인덱스를 사용하여 데이터셋 분할\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "val_dataset = Subset(dataset, val_idx)\n",
        "\n",
        "# 데이터 로더 설정\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "all_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6PoNn87MMURy"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # train_loader에서 첫 번째 배치를 가져옵니다.\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# # 이미지를 표시하기 위한 함수\n",
        "# def imshow(img):\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# # 첫 번째 배치에서 10개의 이미지를 가져와서 시각화합니다.\n",
        "# imshow(torchvision.utils.make_grid(images[:16]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhyF7tfLnFMG",
        "outputId": "ee7c89bb-9528-413c-fba3-e38c16b5d106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import random\n",
        "#random.seed(0)\n",
        "#np.random.seed(0)\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# Seed setting for reproducibility\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "# model_ft = models.vgg19_bn(pretrained=True)\n",
        "# num_ftrs = model_ft.classifier[0].in_features\n",
        "# model_ft.classifier[6] = nn.Linear(num_ftrs, 2)  # 2개의 출력을 가지는 새로운 레이어로 변경\n",
        "\n",
        "model_ft = models.wide_resnet101_2(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "half_in_size = round(num_ftrs/2)\n",
        "layer_width = 1024\n",
        "Num_class=2\n",
        "\n",
        "# Customizable SpinalNet. Supports up to 30 layers.\n",
        "# model_ft.fc = SpinalNet(Input_Size = num_ftrs, Number_of_Split =2, HL_width=512, number_HL=10, Output_Size=2, Activation_Function = nn.ReLU(inplace=True))\n",
        "class SpinalNet(nn.Module):\n",
        "    def __init__(self, Input_Size, Number_of_Split, HL_width, number_HL, Output_Size, Activation_Function):\n",
        "\n",
        "        super(SpinalNet, self).__init__()\n",
        "        Splitted_Input_Size = int(np.round(Input_Size/Number_of_Split))\n",
        "        self.lru = Activation_Function\n",
        "        self.fc1 = nn.Linear(Splitted_Input_Size, HL_width)\n",
        "        if number_HL>1:\n",
        "            self.fc2 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>2:\n",
        "            self.fc3 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>3:\n",
        "            self.fc4 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>4:\n",
        "            self.fc5 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>5:\n",
        "            self.fc6 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>6:\n",
        "            self.fc7 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>7:\n",
        "            self.fc8 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>8:\n",
        "            self.fc9 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>9:\n",
        "            self.fc10 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>10:\n",
        "            self.fc11 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>11:\n",
        "            self.fc12 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>12:\n",
        "            self.fc13 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>13:\n",
        "            self.fc14 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>14:\n",
        "            self.fc15 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>15:\n",
        "            self.fc16 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>16:\n",
        "            self.fc17 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>17:\n",
        "            self.fc18 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>18:\n",
        "            self.fc19 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>19:\n",
        "            self.fc20 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>20:\n",
        "            self.fc21 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>21:\n",
        "            self.fc22 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>22:\n",
        "            self.fc23 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>23:\n",
        "            self.fc24 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>24:\n",
        "            self.fc25 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>25:\n",
        "            self.fc26 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>26:\n",
        "            self.fc27 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>27:\n",
        "            self.fc28 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>28:\n",
        "            self.fc29 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "        if number_HL>29:\n",
        "            self.fc30 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n",
        "\n",
        "        self.fcx = nn.Linear(HL_width*number_HL, Output_Size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_all =x\n",
        "\n",
        "        Splitted_Input_Size = self.fc1.in_features\n",
        "        HL_width = self.fc2.in_features - self.fc1.in_features\n",
        "        number_HL = int(np.round(self.fcx.in_features/HL_width))\n",
        "        length_x_all = number_HL*Splitted_Input_Size\n",
        "\n",
        "        while x_all.size(dim=1) < length_x_all:\n",
        "            x_all = torch.cat([x_all, x],dim=1)\n",
        "\n",
        "        x = self.lru(self.fc1(x_all[:,0:Splitted_Input_Size]))\n",
        "        x_out = x\n",
        "\n",
        "        counter1 = 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc2(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc3(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc4(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc5(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc6(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc7(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc8(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc9(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc10(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc11(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc12(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc13(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc14(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc15(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc16(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc17(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc18(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc19(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc20(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc21(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc22(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc23(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc24(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc25(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc26(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc27(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc28(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc29(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "\n",
        "        counter1 = counter1 + 1\n",
        "        if number_HL>counter1:\n",
        "            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n",
        "            x = self.lru(self.fc30(torch.cat([x_from_all, x], dim=1)))\n",
        "            x_out = torch.cat([x_out, x], dim=1)\n",
        "        #print(\"Size before output layer:\",x_out.size(dim=1))\n",
        "        x = self.fcx(x_out)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T1WN1HOYnHAY"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, dataset_sizes, device, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward and backward\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes['train']\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
        "\n",
        "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            with torch.set_grad_enabled(False):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes['valid']\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes['valid']\n",
        "\n",
        "        print('Valid Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print('model save \\n')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwJs91Bmpday",
        "outputId": "d6edebfa-6a52-4700-c058-13f9c33f8ac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7908, 1.3598], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# class_weights = torch.tensor([2.5,1], dtype=torch.float).to(device)\n",
        "\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SLafD42VnIwa"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "'''\n",
        "Changing the fully connected layer to SpinalNet\n",
        "'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset_sizes = {\n",
        "    'train': len(train_loader.dataset),\n",
        "    'valid': len(val_loader.dataset),\n",
        "    'all': len(all_loader.dataset)\n",
        "}\n",
        "\n",
        "\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "\n",
        "# model_ft.fc = SpinalNet(Input_Size = num_ftrs, Number_of_Split =2, HL_width=1024, number_HL=30, Output_Size=2, Activation_Function = Mish())\n",
        "# model_ft = model_ft.to(device)\n",
        "\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# class_weights = torch.tensor([2.21, 1.83], dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights\n",
        "    )\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.0001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.01)\n",
        "\n",
        "# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, train_loader, val_loader, dataset_sizes, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dO5f3twdPqyf"
      },
      "outputs": [],
      "source": [
        "def only_train_model(model, criterion, optimizer, scheduler, train_loader, dataset_sizes, device, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward and backward\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes['all']\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes['all']\n",
        "\n",
        "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print('model save \\n')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best train Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvwR6IP_P_Xf",
        "outputId": "3653d495-42a4-4676-bf4f-c02c1567379b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2988 Acc: 0.8898\n",
            "Valid Loss: 0.1338 Acc: 0.9569\n",
            "model save \n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "Train Loss: 0.0919 Acc: 0.9782\n",
            "Valid Loss: 0.1187 Acc: 0.9765\n",
            "model save \n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "Train Loss: 0.0650 Acc: 0.9915\n",
            "Valid Loss: 0.1296 Acc: 0.9726\n",
            "Epoch 3/9\n",
            "----------\n",
            "Train Loss: 0.0550 Acc: 0.9932\n",
            "Valid Loss: 0.1276 Acc: 0.9765\n",
            "Epoch 4/9\n",
            "----------\n",
            "Train Loss: 0.0524 Acc: 0.9948\n",
            "Valid Loss: 0.1152 Acc: 0.9706\n",
            "Epoch 5/9\n",
            "----------\n",
            "Train Loss: 0.0475 Acc: 0.9952\n",
            "Valid Loss: 0.1218 Acc: 0.9746\n",
            "Epoch 6/9\n",
            "----------\n",
            "Train Loss: 0.0474 Acc: 0.9946\n",
            "Valid Loss: 0.1112 Acc: 0.9706\n",
            "Epoch 7/9\n",
            "----------\n",
            "Train Loss: 0.0427 Acc: 0.9963\n",
            "Valid Loss: 0.0819 Acc: 0.9824\n",
            "model save \n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "Train Loss: 0.0453 Acc: 0.9950\n",
            "Valid Loss: 0.1138 Acc: 0.9706\n",
            "Epoch 9/9\n",
            "----------\n",
            "Train Loss: 0.0383 Acc: 0.9980\n",
            "Valid Loss: 0.0908 Acc: 0.9765\n",
            "Training complete in 7m 41s\n",
            "Best val Acc: 0.982387\n"
          ]
        }
      ],
      "source": [
        "class_weights = torch.tensor([3,1], dtype=torch.float).to(device)\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights\n",
        "    )\n",
        "\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "model_ft.fc = SpinalNet(Input_Size = num_ftrs, Number_of_Split=2, HL_width=1024, number_HL=30, Output_Size=2, Activation_Function = Mish())\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# model_ft = only_train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, all_loader, dataset_sizes, device, num_epochs=10)\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, train_loader, val_loader, dataset_sizes, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oXkHYrow_Ex2"
      },
      "outputs": [],
      "source": [
        "# 추가 학습\n",
        "# model_ft = only_train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, train_loader, dataset_sizes, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iifY8_Ri7nAg"
      },
      "source": [
        "# 변시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XT_xRdm97qEc"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class SortedImageFolder(VisionDataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(SortedImageFolder, self).__init__(root, transform=transform)\n",
        "        self.images = sorted([os.path.join(root, img) for img in os.listdir(root) if img.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, img_path  # 이미지와 파일 경로를 반환\n",
        "\n",
        "test_data_path = '/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/new_test/test'\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073] , std=[0.26862954, 0.26130258, 0.27577711])\n",
        "    # transforms.Normalize((0.487), (0.267))\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "test_dataset = SortedImageFolder(root=test_data_path, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# 모델을 평가 모드로 설정\n",
        "model_ft.eval()\n",
        "\n",
        "# 예측 결과를 저장할 리스트\n",
        "predictions = []\n",
        "\n",
        "# 테스트 데이터에 대해 예측 수행\n",
        "with torch.no_grad():\n",
        "    for inputs, paths in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model_ft(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predictions.extend(zip(paths, preds.cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLqxJjSR7rWo",
        "outputId": "f4a1fac6-791d-4a6c-ebca-03d35ef2b558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1065"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "preds = []\n",
        "\n",
        "# 파일 이름에 따라 정렬된 예측 결과 출력\n",
        "for path, pred in sorted(predictions, key=lambda x: int(os.path.basename(x[0]).split('.')[0])):\n",
        "    # print(f'File: {os.path.basename(path)}, Prediction: {pred}')\n",
        "    preds.append(pred)\n",
        "\n",
        "preds.count(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N73pVydj7tGJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "submit = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/sample_submission.csv')\n",
        "submit.iloc[:, 1] = preds\n",
        "\n",
        "submit.to_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/result/submit_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z15sakRb7o3Y"
      },
      "source": [
        "# 변끝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jxG2BfQtzg-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "44a051e0-5202-4c95-863c-e7595bac042e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-79776a331478>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# 테스트 데이터에 대해 예측 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-79776a331478>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class SortedImageFolder(VisionDataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(SortedImageFolder, self).__init__(root, transform=transform)\n",
        "        self.images = sorted([os.path.join(root, img) for img in os.listdir(root) if img.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, img_path  # 이미지와 파일 경로를 반환\n",
        "\n",
        "test_data_path = '/content/drive/MyDrive/Pre_processing/test'\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073] , std=[0.26862954, 0.26130258, 0.27577711])\n",
        "    # transforms.Normalize((0.487), (0.267))\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "test_dataset = SortedImageFolder(root=test_data_path, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# 모델을 평가 모드로 설정\n",
        "model_ft.eval()\n",
        "\n",
        "# # 예측 결과를 저장할 리스트\n",
        "# predictions = []\n",
        "\n",
        "# # 테스트 데이터에 대해 예측 수행\n",
        "# with torch.no_grad():\n",
        "#     for inputs, paths in test_loader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         outputs = model_ft(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         predictions.extend(zip(paths, preds.cpu().numpy()))\n",
        "\n",
        "# 예측 확률을 저장할 리스트\n",
        "predictions_probs = []\n",
        "\n",
        "# 테스트 데이터에 대해 예측 수행\n",
        "with torch.no_grad():\n",
        "    for inputs, paths in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model_ft(inputs)\n",
        "\n",
        "        # softmax를 사용하여 확률로 변환\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        predictions_probs.extend(zip(paths, probs.cpu().numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGVj4SaAzrEB"
      },
      "outputs": [],
      "source": [
        "# 클래스 0의 확률을 추출하고 (파일 경로, 확률) 쌍을 저장\n",
        "class_zero_probs = [(path, prob[0]) for path, prob in predictions_probs]\n",
        "\n",
        "# 확률에 따라 내림차순으로 정렬\n",
        "class_zero_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 상위 970개 샘플 선택\n",
        "selected_for_class_zero = set([path for path, _ in class_zero_probs[:970]])\n",
        "\n",
        "# 최종 예측 결과 생성\n",
        "final_predictions = [(path, 0 if path in selected_for_class_zero else 1) for path, _ in predictions_probs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PMOU-p9zr3x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# final_predictions에서 파일 경로와 예측값 추출\n",
        "file_paths, preds = zip(*final_predictions)\n",
        "\n",
        "# 파일명만 추출 (전체 경로에서 파일명만 필요하다면)\n",
        "file_names = [os.path.basename(path) for path in file_paths]\n",
        "\n",
        "# DataFrame 생성\n",
        "submit_df = pd.DataFrame({'Id': file_names, 'Class': preds})\n",
        "\n",
        "# CSV 파일로 저장\n",
        "submit_df.to_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/submit39.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_iikziizs4B",
        "outputId": "426687a5-64ca-4889-ea9b-bbc02cb550f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-89-2a55d828a386>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  submit_df['Id'] = submit_df['Id'].str.replace('.jpg', '').astype(int)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 읽기\n",
        "submit_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/submit39.csv')\n",
        "\n",
        "# 'Id' 컬럼에서 '.jpg' 제거하고 정수로 변환\n",
        "submit_df['Id'] = submit_df['Id'].str.replace('.jpg', '').astype(int)\n",
        "\n",
        "# 내림차순으로 정렬\n",
        "submit_df = submit_df.sort_values(by='Id', ascending=True)\n",
        "\n",
        "# 정렬된 DataFrame을 다시 CSV 파일로 저장\n",
        "submit_df.to_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/submit39.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68_Y8FM40Ym3",
        "outputId": "7ca84d7a-61fc-4a77-ae12-d1b14b45eca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "765"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = []\n",
        "\n",
        "# 파일 이름에 따라 정렬된 예측 결과 출력\n",
        "for path, pred in sorted(predictions, key=lambda x: int(os.path.basename(x[0]).split('.')[0])):\n",
        "    # print(f'File: {os.path.basename(path)}, Prediction: {pred}')\n",
        "    preds.append(pred)\n",
        "\n",
        "preds.count(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuL11ML-0ZTQ",
        "outputId": "ba1af56e-b45a-411e-def8-2a7b72b157fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1063"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.count(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAlhFVfR0aP1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "submit = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/sample_submission.csv')\n",
        "submit.iloc[:, 1] = preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbYbSpYg0bV-"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/result/submit36.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTdyctUHSWUV"
      },
      "outputs": [],
      "source": [
        "# torch.save(model_ft.state_dict(), '/content/drive/MyDrive/Colab Notebooks/2023_HD_DT_2/2023-uou-hd-dt-2-cv/result/model21.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF-9LWsCWoJM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}